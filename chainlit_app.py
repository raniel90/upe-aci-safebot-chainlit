"""
SafeBot - Sistema Inteligente de SeguranÃ§a do Trabalho
Chatbot especializado em NR-06 usando Chainlit e RAG
"""

import os
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain_anthropic import ChatAnthropic
from langchain_openai import OpenAIEmbeddings
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv

import chainlit as cl

# Importar autenticaÃ§Ã£o
from auth import get_user_from_session, get_user_name, get_user_role

# Importar prompts personalizados por role
from prompts import (
    get_instructions_by_role,
    get_welcome_message_by_role,
    get_system_context_by_role,
)

# Carregar variÃ¡veis de ambiente
load_dotenv()

# ConfiguraÃ§Ãµes
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # Ainda necessÃ¡rio para embeddings
PDF_PATH = "data/pdfs/nr-06-atualizada-2022-1.pdf"

# Text splitter para documentos
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=250)

# NOTA: As instruÃ§Ãµes especÃ­ficas por role estÃ£o agora em prompts.py
# Elas sÃ£o carregadas dinamicamente baseado na role do usuÃ¡rio logado


async def load_pdf_knowledge_base() -> Chroma:
    """
    Carrega o PDF da NR-06 e cria a base de conhecimento vetorial
    """
    # Verificar se o arquivo existe
    if not os.path.exists(PDF_PATH):
        raise FileNotFoundError(
            f"PDF da NR-06 nÃ£o encontrado em {PDF_PATH}. "
            f"Por favor, coloque o arquivo PDF no diretÃ³rio data/pdfs/"
        )

    # Carregar PDF
    msg = cl.Message(content="ğŸ”„ Carregando base de conhecimento da NR-06...")
    await msg.send()

    loader = PyPDFLoader(PDF_PATH)
    documents = loader.load()

    # Adicionar metadados aos documentos
    for i, doc in enumerate(documents):
        doc.metadata = {
            "source": "NR-06",
            "document_type": "norma_regulamentadora",
            "nr_number": "06",
            "year": 2022,
            "topic": "equipamentos_protecao_individual",
            "language": "portuguese",
            "page": i + 1,
        }

    # Dividir em chunks
    texts = text_splitter.split_documents(documents)

    msg.content = f"ğŸ“š Processando {len(texts)} seÃ§Ãµes do documento..."
    await msg.update()

    # Criar vector store
    embeddings = OpenAIEmbeddings()
    docsearch = await cl.make_async(Chroma.from_documents)(
        texts,
        embeddings,
        collection_name="safebot_nr06",
        persist_directory="./tmp/chromadb",
    )

    msg.content = "âœ… Base de conhecimento carregada com sucesso!"
    await msg.update()

    return docsearch


@cl.on_chat_start
async def start():
    """
    Inicializa o chat e carrega a base de conhecimento
    """
    # Obter usuÃ¡rio autenticado e sua role
    user = get_user_from_session()
    user_name = get_user_name(user) if user else "UsuÃ¡rio"
    user_role = get_user_role(user) if user else "user"

    # Obter mensagem de boas-vindas personalizada por role
    welcome_msg = get_welcome_message_by_role(user_role, user_name)

    await cl.Message(content=welcome_msg).send()

    # Carregar base de conhecimento
    try:
        docsearch = await load_pdf_knowledge_base()
    except FileNotFoundError as e:
        await cl.Message(
            content=f"âš ï¸ **Aviso:** {str(e)}\n\n"
            "VocÃª ainda pode fazer perguntas, mas as respostas nÃ£o terÃ£o "
            "a base de conhecimento da NR-06."
        ).send()
        # Criar vector store vazio como fallback
        embeddings = OpenAIEmbeddings()
        docsearch = Chroma(
            collection_name="safebot_nr06_empty",
            embedding_function=embeddings,
            persist_directory="./tmp/chromadb",
        )
    except Exception as e:
        await cl.Message(
            content=f"âŒ **Erro ao carregar base de conhecimento:** {str(e)}\n\n"
            "Por favor, verifique sua OPENAI_API_KEY e tente novamente."
        ).send()
        return

    # Configurar memÃ³ria da conversa
    message_history = ChatMessageHistory()
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        output_key="answer",
        chat_memory=message_history,
        return_messages=True,
    )

    # Obter instruÃ§Ãµes e contexto personalizados baseados na role do usuÃ¡rio
    instructions = get_instructions_by_role(user_role)
    system_context = get_system_context_by_role(user_role)

    # Criar prompt template para QA (usando template string, nÃ£o Messages)
    qa_template = f"""â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ REGRAS CRÃTICAS - NUNCA VIOLE ESTAS INSTRUÃ‡Ã•ES              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸ ADERÃŠNCIA OBRIGATÃ“RIA AO CONTEXTO:

1. âœ… VOCÃŠ DEVE usar EXCLUSIVAMENTE as informaÃ§Ãµes presentes no "Contexto da NR-06" abaixo
2. âœ… VOCÃŠ DEVE citar a pÃ¡gina especÃ­fica de onde tirou a informaÃ§Ã£o
3. âŒ Ã‰ PROIBIDO inventar, inferir ou extrapolar informaÃ§Ãµes nÃ£o presentes no contexto
4. âŒ Ã‰ PROIBIDO criar exemplos que nÃ£o estejam explicitamente no documento
5. âŒ NUNCA suponha informaÃ§Ãµes que nÃ£o estÃ£o escritas no contexto fornecido

ğŸ“‹ QUANDO O CONTEXTO NÃƒO CONTÃ‰M A RESPOSTA:

â€¢ Seja TRANSPARENTE: "NÃ£o encontrei informaÃ§Ã£o especÃ­fica sobre [tÃ³pico] na NR-06"
â€¢ Se aplicÃ¡vel, mencione APENAS princÃ­pios gerais que ESTEJAM no contexto fornecido
â€¢ Sugira consultar supervisor/SESMT para casos especÃ­ficos nÃ£o cobertos
â€¢ NUNCA tente responder sem fundamentaÃ§Ã£o no contexto

âœ… EXEMPLO CORRETO:
UsuÃ¡rio: "Qual a cor do capacete para soldador?"
Contexto: [contÃ©m info sobre cores]
Resposta: "Segundo a NR-06 (PÃ¡gina X), o capacete para soldador deve ser [info do contexto]"

âŒ EXEMPLO INCORRETO:
UsuÃ¡rio: "Qual a cor do capacete para soldador?"
Contexto: [NÃƒO contÃ©m info sobre cores]
Resposta: "Normalmente Ã© azul ou amarelo" â† PROIBIDO! Isso Ã© inventar informaÃ§Ã£o!

âœ… RESPOSTA CORRETA quando nÃ£o hÃ¡ info:
"NÃ£o encontrei informaÃ§Ã£o especÃ­fica sobre cores de capacete na NR-06. Para essa dÃºvida especÃ­fica, recomendo consultar seu supervisor ou a equipe de seguranÃ§a (SESMT)."

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{instructions}

{system_context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“– CONTEXTO DA NR-06 (USE APENAS ISTO):
{{context}}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¬ HISTÃ“RICO DA CONVERSA:
{{chat_history}}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â“ PERGUNTA DO USUÃRIO: {{question}}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¤– SUA RESPOSTA (baseada EXCLUSIVAMENTE no contexto acima):"""

    qa_prompt = ChatPromptTemplate.from_template(qa_template)

    # Criar chain com Claude Sonnet 4.5 (versÃ£o mais recente)
    llm = ChatAnthropic(
        model="claude-sonnet-4-5-20250929",
        temperature=0.3,
        max_tokens=4096,
        anthropic_api_key=ANTHROPIC_API_KEY,
    )

    # Configurar retriever com metadados
    retriever = docsearch.as_retriever(
        search_type="similarity",
        search_kwargs={
            "k": 6,  # Aumentado de 4 para 6 documentos para mais contexto relevante
        },
    )

    # Criar chain com prompt customizado
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        memory=memory,
        return_source_documents=True,
        verbose=True,
        combine_docs_chain_kwargs={"prompt": qa_prompt},
    )

    # Armazenar na sessÃ£o do usuÃ¡rio
    cl.user_session.set("chain", chain)
    cl.user_session.set("docsearch", docsearch)
    cl.user_session.set("user_role", user_role)
    cl.user_session.set("instructions", instructions)
    cl.user_session.set("system_context", system_context)


@cl.on_message
async def main(message: cl.Message):
    """
    Processa mensagens do usuÃ¡rio com contexto personalizado por role
    """
    # Recuperar chain e instruÃ§Ãµes da sessÃ£o
    chain = cl.user_session.get("chain")  # type: ConversationalRetrievalChain
    user_role = cl.user_session.get("user_role", "user")

    if not chain:
        await cl.Message(
            content="âŒ **Erro:** SessÃ£o nÃ£o inicializada. Por favor, recarregue a pÃ¡gina."
        ).send()
        return

    # Callback - o streaming Ã© automÃ¡tico quando ChatOpenAI tem streaming=True
    cb = cl.AsyncLangchainCallbackHandler()

    # Processar mensagem com streaming automÃ¡tico
    try:
        res = await chain.acall(message.content, callbacks=[cb])
        answer = res["answer"]
        source_documents = res["source_documents"]  # type: List[Document]

        # Criar elementos de texto para as fontes
        text_elements = []  # type: List[cl.Text]

        if source_documents:
            # Agrupar fontes por pÃ¡gina
            sources_by_page = {}
            for source_doc in source_documents:
                page = source_doc.metadata.get("page", "N/A")
                if page not in sources_by_page:
                    sources_by_page[page] = []
                sources_by_page[page].append(source_doc.page_content)

            # Criar elementos de texto para cada pÃ¡gina
            for page_num, contents in sources_by_page.items():
                source_name = f"ğŸ“„ NR-06 - PÃ¡gina {page_num}"
                combined_content = "\n\n---\n\n".join(contents)
                text_elements.append(
                    cl.Text(content=combined_content, name=source_name, display="side")
                )

            # Adicionar referÃªncias das fontes na resposta
            source_names = [text_el.name for text_el in text_elements]
            if source_names:
                answer += (
                    f"\n\n---\nğŸ“š **Fontes consultadas:** {', '.join(source_names)}"
                )

        # Enviar resposta com fontes
        await cl.Message(content=answer, elements=text_elements).send()

    except Exception as e:
        await cl.Message(
            content=f"âŒ **Erro ao processar sua pergunta:** {str(e)}\n\n"
            "Por favor, tente reformular sua pergunta ou verifique "
            "se sua OPENAI_API_KEY estÃ¡ configurada corretamente."
        ).send()


@cl.on_chat_end
async def end():
    """
    Finaliza a sessÃ£o de chat
    """
    await cl.Message(
        content="ğŸ‘‹ **Obrigado por usar o SafeBot!**\n\n"
        "Lembre-se: A seguranÃ§a no trabalho comeÃ§a com vocÃª. "
        "Volte sempre que precisar! ğŸ›¡ï¸"
    ).send()
